{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5202 Group Assessment - Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://51665ba4a540:4040\n",
       "SparkContext available as 'sc' (version = 2.4.3, master = local[*], app id = local-1558858190018)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{HashingTF, IDF, StringIndexer, RegexTokenizer}\n",
       "import org.apache.spark.ml.feature.StopWordsRemover\n",
       "import org.apache.spark.sql.DataFrame\n",
       "import scala.io.Source\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// import packages\n",
    "import org.apache.spark.ml.feature.{HashingTF, IDF, StringIndexer, RegexTokenizer}\n",
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import scala.io.Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT REDDIT COMMENTS CSVS AND COMBINE INTO SPARK DATAFRAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfPos: org.apache.spark.sql.DataFrame = [id: string, parent_id: string ... 13 more fields]\n",
       "dfNeg: org.apache.spark.sql.DataFrame = [id: string, parent_id: string ... 13 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// set file import options\n",
    "var dfPos = spark.read\n",
    "          .option(\"quote\", \"\\\"\")\n",
    "          .option(\"escape\", \"\\\"\")\n",
    "          .option(\"wholeFile\", true)\n",
    "          .option(\"multiline\",true)\n",
    "          .option(\"header\", true)\n",
    "          .option(\"inferSchema\", \"true\")\n",
    "            .csv(\"./comments_positive.csv\")\n",
    "var dfNeg = spark.read\n",
    "          .option(\"quote\", \"\\\"\")\n",
    "          .option(\"escape\", \"\\\"\")\n",
    "          .option(\"wholeFile\", true)\n",
    "          .option(\"multiline\",true)\n",
    "          .option(\"header\", true)\n",
    "          .option(\"inferSchema\", \"true\")\n",
    "            .csv(\"./comments_negative.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, parent_id: string ... 13 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// combine the pos and neg dataframes together\n",
    "var df = dfPos.union(dfNeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+--------+--------------------+-----+----+------------+----------------+--------------+--------------------+------------+----------+-------------+-----------------------+\n",
      "|     id| parent_id|subreddit_id| link_id|                text|score| ups|      author|controversiality|parent_link_id|         parent_text|parent_score|parent_ups|parent_author|parent_controversiality|\n",
      "+-------+----------+------------+--------+--------------------+-----+----+------------+----------------+--------------+--------------------+------------+----------+-------------+-----------------------+\n",
      "|c092j8m|t1_c092gss|    t5_2qh2p|t3_8eyy3|This isn't Twitte...| 9582|9582|  nraustinii|               0|      t3_8eyy3|     Fucking faggot.|       -7526|     -7526|   Glorificus|                      0|\n",
      "|c4imcva|t1_c4im948|    t5_2qh1i|t3_t0ynr|Well, it is exact...| 9531|9531|     Lynfect|               0|      t3_t0ynr|Elaborate on this...|        3841|      3841|     eeeeevil|                      0|\n",
      "|c0s4nfi|t1_c0s4lje|    t5_2qh1i|t3_cf1n2|In soviet Russia,...| 8545|8545|CapnScumbone|               0|      t3_cf1n2|I don't live in R...|         621|       621|      shady8x|                      0|\n",
      "|c4ini33|t1_c4incln|    t5_2qh1i|t3_t0ynr|\"runin for senitu...| 7430|7430|   [deleted]|               0|      t3_t0ynr|This just made me...|        4651|      4651|      drspg99|                      0|\n",
      "|c4imgel|t1_c4ima2e|    t5_2qh1i|t3_t0ynr|You step motherfu...| 7173|7173|       jbg89|               0|      t3_t0ynr|I have sex with m...|        4251|      4251|       audir8|                      0|\n",
      "+-------+----------+------------+--------+--------------------+-----+----+------------+----------------+--------------+--------------------+------------+----------+-------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// view the data to show everything is working as expected\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REMOVE ROWS WITH SHALLOW TEXT\n",
    "Comments with one or two words are typically low scored  \n",
    "and do not lend insight since they are overwhelmingly\n",
    "made up of extremely common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, parent_id: string ... 13 more fields]\n",
       "res1: Long = 4000000\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// make a copy of the unaltered dataframe\n",
    "val df2 = df\n",
    "// count the number of unaltered rows\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out rows where text char length is below 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, parent_id: string ... 13 more fields]\n",
       "res2: Long = 3991922\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// remove rows with text fields less than 3 characters in length\n",
    "df = df.where(length(col(\"text\")) >= 3)\n",
    "// count the remaining rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRIM THE DATASET TO THE SELECTED COLUMNS AND CREATE NEW DATAFRAME DROP ROWS WITH NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reduceToSelectFields: (loadedCsv: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define function to return dataframe of selected columns\n",
    "def reduceToSelectFields(loadedCsv:org.apache.spark.sql.DataFrame): org.apache.spark.sql.DataFrame = {\n",
    "    var trimmedCol = loadedCsv.col(\"text\")\n",
    "    var newDf = loadedCsv.withColumn(\"comments\",trimmedCol).select(\"id\",\n",
    "                                                                   \"score\",\n",
    "                                                                   \"parent_score\",\n",
    "                                                                   \"subreddit_id\",\n",
    "                                                                   \"comments\")\n",
    "    return newDf.na.drop()\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "temp: org.apache.spark.sql.DataFrame = [id: string, score: int ... 3 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// dataframe of selected columns: text and id \n",
    "val temp = reduceToSelectFields(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOKENIZE THE DATA USING REGEX TOKENIZER FROM MLLIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "regexTokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_ca67bfe91ca4\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define the regex tokenizer\n",
    "val regexTokenizer = new RegexTokenizer()\n",
    "  .setInputCol(\"comments\")\n",
    "  .setOutputCol(\"words\")\n",
    "  .setPattern(\"\\\\w*[-']?\\\\w?\").setGaps(false) // CAPTURE WORDS WITH APOSTROPHES AND HYPHENS INSIDE\n",
    "  .setToLowercase(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordsData: org.apache.spark.sql.DataFrame = [id: string, score: int ... 4 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// apply the tokenizer to the dataframe\n",
    "var wordsData = regexTokenizer.transform(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[org.apache.spark.sql.Row] = Array([WrappedArray(this, isn't, twitter, try, to, comment, on, the, article, and, not, your, current, activities)])\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// view a row to affirm the tokenizer has applied correctly\n",
    "wordsData.select(\"words\").take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REMOVE STOPWORDS USING CUSTOM STOPWORD LIST FROM EDA\n",
    "Use the StopWordsRemover function from ML Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fileLines: List[String] = List(v, abst, na, beginnings, ., m, haven, y, why, ask, said, would, accordingly, overall, somewhere, wouldnt, become, tis, ;, other, adj, itd, selves, was, ll, indeed, 'll, mostly, ol, does, won, same, i've, ,, briefly, anyway, it, cannot, specifying, begins, f, name, mustn, out, _, gotten, over, looking, act, howbeit, everybody, 6, 1, eight, hereby, mg, thru, probably, <, readily, yet, nowhere, my, words, yourselves, seem, wasn, unto, anyways, behind, now, I, :, by, upon, also, do, part, ought, seemed, keep\tkeeps, formerly, brief, co, becoming, enough, hereafter, research, shows, unless, gone, mightn, awfully, /, most, 0f, others, they, whole, used, afterwards, did, ones, for, shes, these, otherwise, usually, didn, necessary, both, such, couldnt, be, whoever,..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// load list of stopwords\n",
    "val fileLines = Source.fromFile(\"./lex_stopwords.txt\").getLines.toList\n",
    "// save stopwords as an array\n",
    "val stpWordList = fileLines.toArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: String = abst\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// view a word from the stopwords array\n",
    "stpWordList(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sWRemover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_a30b7ddf6cfb\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define input and output for stopword removal\n",
    "val sWRemover = new StopWordsRemover()\n",
    "    .setInputCol(\"words\")\n",
    "    .setOutputCol(\"words_stopped\")\n",
    "    .setStopWords(stpWordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordsData: org.apache.spark.sql.DataFrame = [id: string, score: int ... 5 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// remove stopwords\n",
    "wordsData = sWRemover.transform(wordsData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check how rows many have no tokens, then remove these rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Long = 3991922\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// view number of rows\n",
    "wordsData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordsData2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, score: int ... 5 more fields]\n",
       "res6: Long = 3914568\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// remove rows with empty text fields\n",
    "var wordsData2 = wordsData.where(size(col(\"words_stopped\")) > 0)\n",
    "// view the remaining number of rows\n",
    "wordsData2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordsData: org.apache.spark.sql.DataFrame = [id: string, score: int ... 5 more fields]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// update the dataframe\n",
    "wordsData = wordsData2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save negative and positive datsets separately for later EDA use\n",
    "Export to seprate jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "// write the negative dataset to json for later EDA\n",
    "wordsData2.where(col(\"score\") < 0).select(\"score\",\"words_stopped\",\"words\").coalesce(1).write.json(\"neg_import\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "// write the negative dataset to json for later EDA\n",
    "wordsData2.where(col(\"score\") >= 0).select(\"score\",\"words_stopped\",\"words\").coalesce(1).write.json(\"pos_import\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE TERM FREQUENCY VECTORS USING HASHINGTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hashingTF: org.apache.spark.ml.feature.HashingTF = hashingTF_5a9f5ca840fb\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define input and output for tf-idf vectorization\n",
    "val hashingTF = new HashingTF().setInputCol(\"words_stopped\").setOutputCol(\"Features\")\n",
    ".setNumFeatures(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featureData: org.apache.spark.sql.DataFrame = [id: string, score: int ... 6 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// vectorize the tokens\n",
    "val featureData = hashingTF.transform(wordsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Array[org.apache.spark.sql.Row] = Array([c092j8m,9582,-7526,t5_2qh2p,This isn't Twitter: try to comment on the article, and not your current activities.,WrappedArray(this, isn't, twitter, try, to, comment, on, the, article, and, not, your, current, activities),WrappedArray(twitter, comment, article, current, activities),(20000,[1294,3186,4470,6710,16622],[1.0,1.0,1.0,1.0,1.0])])\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// view a row from from the vectorized dataframe\n",
    "featureData.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features[1]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str: String = features[1]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// view a row from the vectorized dataframe in more readable format\n",
    "val str = featureData.col(\"features\")(1).toString()\n",
    "print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------+------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|     id|score|parent_score|subreddit_id|            comments|               words|       words_stopped|            Features|\n",
      "+-------+-----+------------+------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|c092j8m| 9582|       -7526|    t5_2qh2p|This isn't Twitte...|[this, isn't, twi...|[twitter, comment...|(20000,[1294,3186...|\n",
      "+-------+-----+------------+------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featureData.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPORT TO JSON\n",
    "Features to Export:  ID, PARENT_SCORE, SUBREDDIT_IT, SCORE, WORD TOKENS (WORDS_STOPPED) AND TF VECTOR HASHES  \n",
    "**NOTE**: The following cells create a directory in which to hold the ouput json file.  \n",
    "YOU MUST DELETE THE DIRECTORY TO **RE-RUN** THE CODE HERE ELSE AN ERROR WILL OCCUR.  \n",
    "**NOTE**: The json file created here will be uploaded to Google drive.  \n",
    "Link provided in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " path file:/home/FIT5202_Group_Project/reddit-comment-score-prediction/output_dir already exists.;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: path file:/home/FIT5202_Group_Project/reddit-comment-score-prediction/output_dir already exists.;",
      "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)",
      "  at org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:545)",
      "  ... 36 elided",
      ""
     ]
    }
   ],
   "source": [
    "// export the vectorized dataframe to json\n",
    "featureData.select(\"id\", \"parent_score\",\"subreddit_id\",\"score\",\"words_stopped\", \"Features\").coalesce(1).write.json(\"output_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// view the dimensions of the contents being exported to JSON\n",
    "featureData.select(\"id\", \"parent_score\",\"subreddit_id\",\"score\",\"words_stopped\", \"Features\").take(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
