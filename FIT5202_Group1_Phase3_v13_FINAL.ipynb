{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5202 Group 1 Phase 3\n",
    "Alexis Illig  \n",
    "Robert Allan  \n",
    "Nathan Davis  \n",
    "Enzo Reyes  \n",
    "Ninette Macalalad  \n",
    "Enviroment: Jupyter notebook with spylon-kernel, scala version 2.4.3  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set memory parameters before initializing Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.num_executors = 1\n",
    "launcher.executor_cores = 6\n",
    "launcher.driver_memory = '10g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://1dfa34b9cd3c:4040\n",
       "SparkContext available as 'sc' (version = 2.4.3, master = local[*], app id = local-1559910304267)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6f80083f\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// initialize scala\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// set error log level to less verbose\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.time.LocalDateTime\n",
       "import java.io.{File, PrintWriter}\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.functions.rand\n",
       "import org.apache.spark.ml.linalg.Vector\n",
       "import org.apache.spark.ml.feature.{HashingTF, IDF, VectorAssembler, StringIndexer, OneHotEncoderEstimator}\n",
       "import org.apache.spark.ml.classification.{LogisticRegression, NaiveBayes, RandomForestClassificationModel, RandomForestClassifier}\n",
       "import org.apache.spark.ml.classification.{DecisionTreeClassifier, DecisionTreeClassificationModel, MultilayerPerceptronClassifier}\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.time.LocalDateTime\n",
    "import java.io.{File, PrintWriter}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.functions.rand\n",
    "import org.apache.spark.ml.linalg.{Vector}\n",
    "import org.apache.spark.ml.feature.{HashingTF, IDF, VectorAssembler, StringIndexer, OneHotEncoderEstimator}\n",
    "import org.apache.spark.ml.classification.{LogisticRegression, NaiveBayes, RandomForestClassificationModel, RandomForestClassifier}\n",
    "import org.apache.spark.ml.classification.{DecisionTreeClassifier, DecisionTreeClassificationModel, MultilayerPerceptronClassifier}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Hour: 12\n",
      "Current Minute: 25\n"
     ]
    }
   ],
   "source": [
    "// get current time\n",
    "println(\"Current Hour: \" + LocalDateTime.now().getHour() + \"\\nCurrent Minute: \" + LocalDateTime.now().getMinute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getListOfFiles: (dir: java.io.File, extensions: List[String])List[java.io.File]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Function to search a directory and return a list of files based on filetype\n",
    "def getListOfFiles(dir: File, extensions: List[String]): List[File] = {\n",
    "    dir.listFiles.filter(_.isFile).toList.filter { file =>\n",
    "        extensions.exists(file.getName.endsWith(_))\n",
    "    }\n",
    "}\n",
    "//Source: https://alvinalexander.com/scala/how-to-list-files-in-directory-filter-names-scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics_by_label: (trsum: org.apache.spark.ml.classification.LogisticRegressionTrainingSummary)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Function to return the metrics for each label in a dataframe \n",
    "def metrics_by_label(trsum: org.apache.spark.ml.classification.LogisticRegressionTrainingSummary): org.apache.spark.sql.DataFrame = {\n",
    "    // false positives\n",
    "    var fp = sc.parallelize(trsum.falsePositiveRateByLabel.zipWithIndex).toDF(\"False Positive Rate\", \"Label\")\n",
    "    // true positives\n",
    "    var tp = sc.parallelize(trsum.truePositiveRateByLabel.zipWithIndex).toDF(\"True Positive Rate\", \"Label\") \n",
    "    // precision\n",
    "    var pc = sc.parallelize(trsum.precisionByLabel.zipWithIndex).toDF(\"Precision\", \"Label\") \n",
    "     // recall\n",
    "    var rc = sc.parallelize(trsum.recallByLabel.zipWithIndex).toDF(\"Recall\", \"Label\")   \n",
    "    // f-measure\n",
    "    var fm = sc.parallelize(trsum.fMeasureByLabel.zipWithIndex).toDF(\"F-Measure\", \"Label\")    \n",
    "    \n",
    "    // merge dataframes\n",
    "    var metrics1 = fp.join(tp, \"Label\").join(pc, \"Label\").join(rc, \"Label\").join(fm, \"Label\")\n",
    "\n",
    "    return metrics1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics_all: (trsum: org.apache.spark.ml.classification.LogisticRegressionTrainingSummary)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Function to return the total metrics in dataframe\n",
    "def metrics_all(trsum: org.apache.spark.ml.classification.LogisticRegressionTrainingSummary): org.apache.spark.sql.DataFrame = {\n",
    "    var temp_arr = Array[Double](trsum.accuracy, \n",
    "                                 trsum.weightedFalsePositiveRate, \n",
    "                                 trsum.weightedTruePositiveRate, \n",
    "                                 trsum.weightedFMeasure, \n",
    "                                 trsum.weightedPrecision, \n",
    "                                 trsum.weightedRecall)\n",
    "    val lab_arr = Array[String](\"Accuracy\",\n",
    "                                \"False Positive Rate\",\n",
    "                                \"True Positive Rate\",\n",
    "                                \"F-Measure\", \n",
    "                                \"Precision\", \n",
    "                                \"Recall\")\n",
    "    var metrics2 = sc.parallelize(lab_arr zip temp_arr).toDF(\"Metric\",\"Values\")\n",
    "    return metrics2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Files\n",
    "ATTENTION: Please refer to the Phase 3 report for a link to the \"output_dir2\" folder referenced in the cell below.  \n",
    "You will need to download this folder into the same directroy as this ipynb file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "okFileExtensions: List[String] = List(json)\n",
       "myFiles: List[java.io.File] = List(./output_dir2/part-00000-1be6ed82-1dd3-437f-ac0a-7b6b03ddc15c-c000.json)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// get json file name\n",
    "val okFileExtensions = List(\"json\")\n",
    "val myFiles = getListOfFiles(new File(\"./output_dir2\"), okFileExtensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@733e4845\n",
       "df_both: org.apache.spark.sql.DataFrame = [id: string, parent_score: bigint ... 3 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// read the json file into a spark dataframe\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)    // create an SQL Context object\n",
    "var df_both = sqlContext.read.json(myFiles(0).toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "// rename column if need be\n",
    "if (df_both.columns.contains(\"words_stopped2\")){\n",
    "    df_both = df_both.withColumnRenamed(\"words_stopped2\",\"words_stopped\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_both2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, parent_score: bigint ... 3 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// shuffle the dataset rows in case of class clustering\n",
    "val df_both2 = df_both.orderBy(rand(seed=10L))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-----+------------+--------------------+\n",
      "|     id|parent_score|score|subreddit_id|       words_stopped|\n",
      "+-------+------------+-----+------------+--------------------+\n",
      "|c092j8m|       -7526| 9582|    t5_2qh2p|[twitter, comment...|\n",
      "|c4imcva|        3841| 9531|    t5_2qh1i|[well, exactly, s...|\n",
      "|c0s4nfi|         621| 8545|    t5_2qh1i|[soviet, russia, ...|\n",
      "|c4ini33|        4651| 7430|    t5_2qh1i|       [runin, yolo]|\n",
      "|c4imgel|        4251| 7173|    t5_2qh1i|[step, motherfucker]|\n",
      "+-------+------------+-----+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// view the data\n",
    "df_both.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF\n",
    "Calculate Term Frequencies (TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfModel: org.apache.spark.ml.feature.HashingTF = hashingTF_c18490e1469d\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tfModel = new HashingTF().setInputCol(\"words_stopped\").setOutputCol(\"TF\").setNumFeatures(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_both3: org.apache.spark.sql.DataFrame = [id: string, parent_score: bigint ... 4 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_both3 = tfModel.transform(df_both2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF\n",
    "Calculate the Inverse Document Frequency (IDF) vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idf: org.apache.spark.ml.feature.IDF = idf_a95f8a33f472\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define the idf vectorizer model\n",
    "var idf = new IDF().setInputCol(\"TF\").setOutputCol(\"TF-IDF\").setMinDocFreq(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idfModel: org.apache.spark.ml.feature.IDFModel = idf_a95f8a33f472\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// compute the idf vectors\n",
    "val idfModel = idf.fit(df_both3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_both3b: org.apache.spark.sql.DataFrame = [id: string, parent_score: bigint ... 5 more fields]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// scale the tf's by the idf's\n",
    "val df_both3b = idfModel.transform(df_both3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-----+------------+--------------------+--------------------+--------------------+\n",
      "|     id|parent_score|score|subreddit_id|       words_stopped|                  TF|              TF-IDF|\n",
      "+-------+------------+-----+------------+--------------------+--------------------+--------------------+\n",
      "|c6m07b0|         514|  238|    t5_2qh33|     [dolphinplasty]|   (200,[117],[1.0])|(200,[117],[3.364...|\n",
      "|c3nxcbf|          22|   82|    t5_2qzb6|[voice, venture, ...|(200,[19,91,101,1...|(200,[19,91,101,1...|\n",
      "+-------+------------+-----+------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// view data\n",
    "df_both3b.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorize the Scores\n",
    "Create classes by binning the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "col_id: org.apache.spark.sql.Column = score\n",
       "df_both4: org.apache.spark.sql.DataFrame = [id: string, parent_score: bigint ... 6 more fields]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val col_id = col(\"score\")\n",
    "// var col_id = col(\"parent_score\")  // sanity check\n",
    "var df_both4 = df_both3b.withColumn(\"label\", \n",
    "                               when(col_id < -1000  , 1)\n",
    "                               .when(col_id >= -1000 && col_id < -100 , 2)\n",
    "                               .when(col_id >= -100 && col_id < 0, 3)\n",
    "                               .when(col_id >= 0 && col_id <= 100, 4)\n",
    "                               .when(col_id > 100 && col_id <= 1000,5)\n",
    "                               .when(col_id > 1000, 6)\n",
    "                               .otherwise(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-----+------------+--------------------+--------------------+--------------------+-----+\n",
      "|     id|parent_score|score|subreddit_id|       words_stopped|                  TF|              TF-IDF|label|\n",
      "+-------+------------+-----+------------+--------------------+--------------------+--------------------+-----+\n",
      "|c6m07b0|         514|  238|    t5_2qh33|     [dolphinplasty]|   (200,[117],[1.0])|(200,[117],[3.364...|    5|\n",
      "|c3nxcbf|          22|   82|    t5_2qzb6|[voice, venture, ...|(200,[19,91,101,1...|(200,[19,91,101,1...|    4|\n",
      "|cnm811v|          76|  -19|    t5_2qh61|[well, making, re...|(200,[0,1,88,103,...|(200,[0,1,88,103,...|    3|\n",
      "|c3ouklj|          88|  120|    t5_2qh03|[install, norton,...|(200,[58,94,157],...|(200,[58,94,157],...|    5|\n",
      "|c40sx8i|           7|   -9|    t5_2qh4i|[personal, opinio...|(200,[2,13,38,55,...|(200,[2,13,38,55,...|    3|\n",
      "+-------+------------+-----+------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// view the data\n",
    "df_both4.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Reduction\n",
    "Create two combinations of the tf-idf vectors and other features for model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combination 1\n",
    "Add parent_score as an extra dimension to the tf-idf vectors and store as a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_0688f9c7d26d\n",
       "df_both4: org.apache.spark.sql.DataFrame = [id: string, parent_score: bigint ... 7 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define vector assembler\n",
    "val assembler = new VectorAssembler()\n",
    "    .setInputCols(Array(\"parent_score\", \"TF-IDF\"))\n",
    "    .setOutputCol(\"comboFeatures1\")\n",
    "\n",
    "// apply assembler to the data\n",
    "df_both4 = assembler.transform(df_both4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combination 2\n",
    "Convert subreddit_id from strings to numerical indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_f8f4fe16021d\n",
       "df_both4: org.apache.spark.sql.DataFrame = [id: string, parent_score: bigint ... 8 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define indexer\n",
    "val indexer = new StringIndexer()\n",
    "  .setInputCol(\"subreddit_id\")\n",
    "  .setOutputCol(\"subreddit_id_int\")\n",
    "\n",
    "// apply indexer to the data\n",
    "df_both4 = indexer.fit(df_both4).transform(df_both4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply One-Hot Encoding to the indexed subreddit id's since that column is linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_4bd46c16ca13\n",
       "df_both4: org.apache.spark.sql.DataFrame = [id: string, parent_score: bigint ... 9 more fields]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define one-hot encoder\n",
    "val encoder = new OneHotEncoderEstimator()\n",
    "  .setInputCols(Array(\"subreddit_id_int\"))\n",
    "  .setOutputCols(Array(\"subreddit_idVec\"))\n",
    "\n",
    "// apply one-hot encoding to the data\n",
    "df_both4 = encoder.fit(df_both4).transform(df_both4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the one-hot encoded subreddit ids as an extra dimension to the tf-idf vectors and store as a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_960c0a403115\n",
       "df_both4: org.apache.spark.sql.DataFrame = [id: string, parent_score: bigint ... 10 more fields]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define vector assembler\n",
    "val assembler = new VectorAssembler()\n",
    "    .setInputCols(Array(\"parent_score\", \"TF-IDF\",\"subreddit_idVec\")) //,\"subreddit_id_int\" \n",
    "    .setOutputCol(\"comboFeatures2\")\n",
    "\n",
    "// apply assembler to the data\n",
    "df_both4 = assembler.transform(df_both4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_workingSet: org.apache.spark.sql.DataFrame = [id: string, parent_score: bigint ... 10 more fields]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define the one-hot encoded dataframe as new variable\n",
    "val df_workingSet = df_both4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Testing\n",
    "Test multiple sets of feature vectors via logistic regression in order to determine the most suitable set of features for selection.  \n",
    "See the report for discussion of logistic regression as the choice model for feature testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Hour: 12\n",
      "Current Minute: 26\n"
     ]
    }
   ],
   "source": [
    "// get current time\n",
    "println(\"Current Hour: \" + LocalDateTime.now().getHour() + \"\\nCurrent Minute: \" + LocalDateTime.now().getMinute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [Model: string, F-Measure: double]\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define an 'empty' dataframe in which to collect metric results from each model\n",
    "var results =  List((\"Log Reg 1\", 0.0), \n",
    "                (\"Log Reg 2\", 0.0),\n",
    "                (\"Log Reg 3\", 0.0),\n",
    "                (\"Log Reg 4\", 0.0)).toDF(\"Model\", \"F-Measure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|Model    |F-Measure|\n",
      "+---------+---------+\n",
      "|Log Reg 1|0.25     |\n",
      "|Log Reg 2|0.0      |\n",
      "|Log Reg 3|0.0      |\n",
      "|Log Reg 4|0.0      |\n",
      "+---------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [Model: string, F-Measure: double]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = results.withColumn(\"F-Measure\", when(col(\"Model\").equalTo(\"Log Reg 1\"), \n",
    "                                               0.25).otherwise(col(\"F-Measure\"))) \n",
    "results.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression 1\n",
    "Use the term frequencies only as the modelling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_0a4bdb773742\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define logistic regression model\n",
    "val lr = new LogisticRegression()\n",
    "    .setMaxIter(100)  \n",
    "    .setRegParam(1e-5)\n",
    "    .setElasticNetParam(0)\n",
    "    .setLabelCol(\"label\")\n",
    "    .setFeaturesCol(\"TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrModel: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_0a4bdb773742, numClasses = 7, numFeatures = 200\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// train the model\n",
    "val lrModel = lr.fit(df_workingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainingSummary: org.apache.spark.ml.classification.LogisticRegressionTrainingSummary = org.apache.spark.ml.classification.LogisticRegressionTrainingSummaryImpl@47cc0191\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// get metrics\n",
    "val trainingSummary = lrModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "|Label| False Positive Rate|  True Positive Rate|          Precision|              Recall|           F-Measure|\n",
      "+-----+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "|    1|4.644463721835843E-6|1.115697869017070...|0.05263157894736842|1.115697869017070...| 2.22667557336896E-4|\n",
      "|    3|6.705818329064822E-5|1.060349199819612...| 0.2852233676975945|1.060349199819612...|2.119910299699126...|\n",
      "|    5|9.618969231257178E-6|1.579820427078122E-4|0.13953488372093023|1.579820427078122E-4|3.156067539845352...|\n",
      "|    4|0.017070099393040605|0.024720710576786886| 0.3705172367753478|0.024720710576786886|0.046349038271234815|\n",
      "|    2|  0.9755509262650316|  0.9857579951165537| 0.5000199512035564|  0.9857579951165537|  0.6634890036394443|\n",
      "|    0|                 0.0|                 0.0|                0.0|                 0.0|                 0.0|\n",
      "+-----+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// view metrics by label\n",
    "metrics_by_label(trainingSummary).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|             Metric|             Values|\n",
      "+-------------------+-------------------+\n",
      "|           Accuracy| 0.4975007368945398|\n",
      "|False Positive Rate| 0.4902030841688557|\n",
      "| True Positive Rate| 0.4975007368945397|\n",
      "|          F-Measure| 0.3434718325082306|\n",
      "|          Precision|0.41475334214000564|\n",
      "|             Recall| 0.4975007368945397|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// view metrics total\n",
    "metrics_all(trainingSummary).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [Model: string, F-Measure: double]\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//update results dataframe\n",
    "results = results.withColumn(\"F-Measure\", when(col(\"Model\").equalTo(\"Log Reg 1\"), \n",
    "                                               metrics_all(trainingSummary).collect()(3)(1)\n",
    "                                              ).otherwise(col(\"F-Measure\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Hour: 12\n",
      "Current Minute: 29\n"
     ]
    }
   ],
   "source": [
    "// get current time\n",
    "println(\"Current Hour: \" + LocalDateTime.now().getHour() + \"\\nCurrent Minute: \" + LocalDateTime.now().getMinute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression 2\n",
    "Use the original tf-idf vectors as the modelling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_c6cbf1f25c7f\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define logistic regression model\n",
    "val lr = new LogisticRegression()\n",
    "    .setMaxIter(100)  \n",
    "    .setRegParam(1e-5)\n",
    "    .setElasticNetParam(0)\n",
    "    .setLabelCol(\"label\")\n",
    "    .setFeaturesCol(\"TF-IDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrModel: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_c6cbf1f25c7f, numClasses = 7, numFeatures = 200\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// train the model\n",
    "val lrModel = lr.fit(df_workingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainingSummary: org.apache.spark.ml.classification.LogisticRegressionTrainingSummary = org.apache.spark.ml.classification.LogisticRegressionTrainingSummaryImpl@73f8a374\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// get metrics\n",
    "val trainingSummary = lrModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "|Label| False Positive Rate|  True Positive Rate|          Precision|              Recall|           F-Measure|\n",
      "+-----+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "|    1|4.644463721835843E-6|1.115697869017070...|0.05263157894736842|1.115697869017070...| 2.22667557336896E-4|\n",
      "|    3|6.705818329064822E-5|1.060349199819612...| 0.2852233676975945|1.060349199819612...|2.119910299699126...|\n",
      "|    5|9.618969231257178E-6|1.579820427078122E-4|0.13953488372093023|1.579820427078122E-4|3.156067539845352...|\n",
      "|    4|0.017070099393040605|0.024720710576786886| 0.3705172367753478|0.024720710576786886|0.046349038271234815|\n",
      "|    2|  0.9755509262650316|  0.9857579951165537| 0.5000199512035564|  0.9857579951165537|  0.6634890036394443|\n",
      "|    0|                 0.0|                 0.0|                0.0|                 0.0|                 0.0|\n",
      "+-----+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// view metrics by label\n",
    "metrics_by_label(trainingSummary).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|             Metric|             Values|\n",
      "+-------------------+-------------------+\n",
      "|           Accuracy| 0.4975007368945398|\n",
      "|False Positive Rate| 0.4902030841688557|\n",
      "| True Positive Rate| 0.4975007368945397|\n",
      "|          F-Measure| 0.3434718325082306|\n",
      "|          Precision|0.41475334214000564|\n",
      "|             Recall| 0.4975007368945397|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// view metrics total\n",
    "metrics_all(trainingSummary).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [Model: string, F-Measure: double]\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//update results dataframe\n",
    "results = results.withColumn(\"F-Measure\", when(col(\"Model\").equalTo(\"Log Reg 2\"), \n",
    "                                               metrics_all(trainingSummary).collect()(3)(1)\n",
    "                                              ).otherwise(col(\"F-Measure\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Hour: 12\n",
      "Current Minute: 32\n"
     ]
    }
   ],
   "source": [
    "// get current time\n",
    "println(\"Current Hour: \" + LocalDateTime.now().getHour() + \"\\nCurrent Minute: \" + LocalDateTime.now().getMinute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression 3\n",
    "Use the first combination tf-idf vectors as the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_9ccd0df687f0\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define logistic regression model\n",
    "val lr = new LogisticRegression()\n",
    "    .setMaxIter(100)  \n",
    "    .setRegParam(1e-5)\n",
    "    .setElasticNetParam(0)\n",
    "    .setLabelCol(\"label\")\n",
    "    .setFeaturesCol(\"comboFeatures1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrModel: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_9ccd0df687f0, numClasses = 7, numFeatures = 201\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// train the model\n",
    "val lrModel = lr.fit(df_workingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainingSummary: org.apache.spark.ml.classification.LogisticRegressionTrainingSummary = org.apache.spark.ml.classification.LogisticRegressionTrainingSummaryImpl@468bf15c\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// get metrics\n",
    "val trainingSummary = lrModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "|Label| False Positive Rate|  True Positive Rate|          Precision|              Recall|           F-Measure|\n",
      "+-----+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "|    1|4.902489484160057E-6|1.115697869017070...|               0.05|1.115697869017070...|2.226427696760547...|\n",
      "|    3|1.631319266589807...|2.836114727228362E-4|0.30494505494505497|2.836114727228362E-4|5.666958949008857E-4|\n",
      "|    5|0.001405149424187...| 0.08612654361620896|0.37701705855232825| 0.08612654361620896| 0.14022076947808382|\n",
      "|    4|  0.1246163961586755|  0.5494552286204737| 0.6418437576093087|  0.5494552286204737|  0.5920670002279734|\n",
      "|    2|  0.5644677081375176|   0.937830251076211| 0.6218380324536302|   0.937830251076211|  0.7478237831250572|\n",
      "|    0|                 0.0|                 0.0|                0.0|                 0.0|                 0.0|\n",
      "+-----+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// view metrics by label\n",
    "metrics_by_label(trainingSummary).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|             Metric|            Values|\n",
      "+-------------------+------------------+\n",
      "|           Accuracy|0.6261780980784107|\n",
      "|False Positive Rate|0.3168353862149347|\n",
      "| True Positive Rate|0.6261780980784106|\n",
      "|          F-Measure|0.5445658925646598|\n",
      "|          Precision|0.5600473537533193|\n",
      "|             Recall|0.6261780980784106|\n",
      "+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// view metrics total\n",
    "metrics_all(trainingSummary).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [Model: string, F-Measure: double]\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//update results dataframe\n",
    "results = results.withColumn(\"F-Measure\", when(col(\"Model\").equalTo(\"Log Reg 3\"), \n",
    "                                               metrics_all(trainingSummary).collect()(3)(1)\n",
    "                                              ).otherwise(col(\"F-Measure\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Hour: 12\n",
      "Current Minute: 35\n"
     ]
    }
   ],
   "source": [
    "// get current time\n",
    "println(\"Current Hour: \" + LocalDateTime.now().getHour() + \"\\nCurrent Minute: \" + LocalDateTime.now().getMinute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression 4\n",
    "Use the second combination tf-idf vectors as the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_b08b7f4fe890\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define logistic regression model\n",
    "val lr = new LogisticRegression()\n",
    "    .setMaxIter(100)  /// upped to 1000\n",
    "    .setRegParam(1e-5)\n",
    "    .setElasticNetParam(0)\n",
    "    .setLabelCol(\"label\")\n",
    "    .setFeaturesCol(\"comboFeatures2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrModel: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_b08b7f4fe890, numClasses = 7, numFeatures = 6093\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// train the model\n",
    "val lrModel = lr.fit(df_workingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainingSummary: org.apache.spark.ml.classification.LogisticRegressionTrainingSummary = org.apache.spark.ml.classification.LogisticRegressionTrainingSummaryImpl@19f28c24\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// get metrics\n",
    "val trainingSummary = lrModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "|Label| False Positive Rate|  True Positive Rate|          Precision|              Recall|           F-Measure|\n",
      "+-----+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "|    1|5.934592533456910...|0.001561977016623...| 0.3783783783783784|0.001561977016623...|0.003111111111111111|\n",
      "|    3|0.005096099534977...| 0.01756219331315689| 0.4651485416525682| 0.01756219331315689| 0.03384647581295434|\n",
      "|    5|0.001348475497365...| 0.09086600489744333|0.39951377633711505| 0.09086600489744333| 0.14805757556256302|\n",
      "|    4| 0.15797454275291062|   0.629426295296286| 0.6182349842768672|   0.629426295296286|  0.6237804477014395|\n",
      "|    2| 0.47038395167143626|   0.923839767482541| 0.6603065605194025|   0.923839767482541|  0.7701529190259158|\n",
      "|    0|                 0.0|                 0.0|                0.0|                 0.0|                 0.0|\n",
      "+-----+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// view metrics by label\n",
    "metrics_by_label(trainingSummary).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|             Metric|            Values|\n",
      "+-------------------+------------------+\n",
      "|           Accuracy|0.6458609695601415|\n",
      "|False Positive Rate|0.2806699736093649|\n",
      "| True Positive Rate|0.6458609695601416|\n",
      "|          F-Measure|0.5716269370365401|\n",
      "|          Precision|0.6056193435074274|\n",
      "|             Recall|0.6458609695601416|\n",
      "+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// view metrics total\n",
    "metrics_all(trainingSummary).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [Model: string, F-Measure: double]\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//update results dataframe\n",
    "results = results.withColumn(\"F-Measure\", when(col(\"Model\").equalTo(\"Log Reg 4\"), \n",
    "                                               metrics_all(trainingSummary).collect()(3)(1)\n",
    "                                              ).otherwise(col(\"F-Measure\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Hour: 12\n",
      "Current Minute: 38\n"
     ]
    }
   ],
   "source": [
    "// get current time\n",
    "println(\"Current Hour: \" + LocalDateTime.now().getHour() + \"\\nCurrent Minute: \" + LocalDateTime.now().getMinute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|Model    |F-Measure         |\n",
      "+---------+------------------+\n",
      "|Log Reg 1|0.3434718325082306|\n",
      "|Log Reg 2|0.3434718325082306|\n",
      "|Log Reg 3|0.5445658925646598|\n",
      "|Log Reg 4|0.5716269370365401|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// view the model results\n",
    "results.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the feature sets without expanded dimensions (1 and 2) performed similiarly due to using such a low number of features (200)\n",
    "* the feature vector sets with extra dimensions (3 and 4) faired better in both accuracy and f-score than than the unexpmanded sets\n",
    "* the second combination of tf-idf vectors with extra dimensions of parent comment score and subreddit id resulted in the best accuracy and f-score  \n",
    "\n",
    "However,  \n",
    "* the second combination only faired better than the first combination by 0.01% f-score, the most relevant metric\n",
    "* the second combination also took significantly more time to run than the first combination  \n",
    "* the performance improvement of the second combination over the first is not significant enough to justify the increase in processing time  \n",
    "\n",
    "**Feature set combination 1 is therefore chosen as the most suitable feature set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "**Models tested:**  \n",
    "Logistic Regression with increased iterations  \n",
    "Random Forest   \n",
    "Multilayer Perceptron Classifier (neural net)  \n",
    "Decision Tree  \n",
    "Naive Bayes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [Model: string, F-Measure: double]\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define an 'empty' dataframe in which to collect metric results from each model\n",
    "var results =  List((\"Logistic Regression\", 0.0), \n",
    "                (\"Random Forest\", 0.0),\n",
    "                (\"Multilayer Perceptron Classifier\", 0.0),\n",
    "                (\"Decision Tree\", 0.0),\n",
    "                (\"Naive Bayes\", 0.0)).toDF(\"Model\", \"F-Measure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression \n",
    "This model differs from those in the feature selection section by increasing the iteration parameter from 100 to 1000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_051f1d90ea0f\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define logistic regression model\n",
    "val lr = new LogisticRegression()\n",
    "    .setMaxIter(1000)  \n",
    "    .setRegParam(1e-5)\n",
    "    .setElasticNetParam(0)\n",
    "    .setLabelCol(\"label\")\n",
    "    .setFeaturesCol(\"comboFeatures1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrModel: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_051f1d90ea0f, numClasses = 7, numFeatures = 201\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// train the model\n",
    "val lrModel = lr.fit(df_workingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainingSummary: org.apache.spark.ml.classification.LogisticRegressionTrainingSummary = org.apache.spark.ml.classification.LogisticRegressionTrainingSummaryImpl@14a81317\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// get metrics\n",
    "val trainingSummary = lrModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Label| False Positive Rate|  True Positive Rate|           Precision|              Recall|           F-Measure|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    1| 5.16051524648427E-6|1.115697869017070...|0.047619047619047616|1.115697869017070...|2.226179875333927E-4|\n",
      "|    3|1.621647413230579...|2.797788852536087E-4|  0.3033240997229917|2.797788852536087E-4|5.590421234410958E-4|\n",
      "|    5|0.001417628086974...|   0.086653150425235|  0.3763723696248856|   0.086653150425235| 0.14087280354429296|\n",
      "|    4| 0.12462870623947307|  0.5495060044664929|  0.6418422928252964|  0.5495060044664929|  0.5920958541340119|\n",
      "|    2|  0.5643995834662959|  0.9378193828723317|  0.6218636891968456|  0.9378193828723317|  0.7478388804458737|\n",
      "|    0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// view metrics by label\n",
    "metrics_by_label(trainingSummary).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|             Metric|             Values|\n",
      "+-------------------+-------------------+\n",
      "|           Accuracy| 0.6261917418899768|\n",
      "|False Positive Rate|0.31680498493928955|\n",
      "| True Positive Rate| 0.6261917418899767|\n",
      "|          F-Measure| 0.5445865731926203|\n",
      "|          Precision| 0.5597212628084544|\n",
      "|             Recall| 0.6261917418899767|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// view metrics total\n",
    "metrics_all(trainingSummary).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [Model: string, F-Measure: double]\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//update results dataframe\n",
    "results = results.withColumn(\"F-Measure\", when(col(\"Model\").equalTo(\"Logistic Regression\"), \n",
    "                                               metrics_all(trainingSummary).collect()(3)(1)\n",
    "                                              ).otherwise(col(\"F-Measure\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Hour: 12\n",
      "Current Minute: 41\n"
     ]
    }
   ],
   "source": [
    "// get current time\n",
    "println(\"Current Hour: \" + LocalDateTime.now().getHour() + \"\\nCurrent Minute: \" + LocalDateTime.now().getMinute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_177124423758\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define random forest model\n",
    "val rf = new RandomForestClassifier()\n",
    "    .setLabelCol(\"label\")\n",
    "    .setFeaturesCol(\"comboFeatures1\")\n",
    "    .setNumTrees(50)\n",
    "    .setSubsamplingRate(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.classification.RandomForestClassificationModel = RandomForestClassificationModel (uid=rfc_177124423758) with 50 trees\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// train the model\n",
    "val model = rf.fit(df_workingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [id: string, parent_score: bigint ... 13 more fields]\n"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// get prediction values\n",
    "val predictions = model.transform(df_workingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_2323c1a81c3d\n"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// compute metric\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "    .setLabelCol(\"label\")\n",
    "    .setPredictionCol(\"prediction\")\n",
    "    .setMetricName(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Measure: 0.3560933182077977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "myMetric: Double = 0.3560933182077977\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// get metric\n",
    "val myMetric = evaluator.evaluate(predictions)\n",
    "\n",
    "// view metric\n",
    "println(\"F-Measure: \" +  myMetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [Model: string, F-Measure: double]\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//update results dataframe\n",
    "results = results.withColumn(\"F-Measure\", when(col(\"Model\").equalTo(\"Random Forest\"), \n",
    "                                               myMetric).otherwise(col(\"F-Measure\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Hour: 12\n",
      "Current Minute: 47\n"
     ]
    }
   ],
   "source": [
    "// get current time\n",
    "println(\"Current Hour: \" + LocalDateTime.now().getHour() + \"\\nCurrent Minute: \" + LocalDateTime.now().getMinute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron Classifier\n",
    "A feedforward artificial neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specify layers for the neural network:**\n",
    "  * input layer of size 201 (features), \n",
    "  * hidden layers of sizes 30 and 30,\n",
    "  * output layer of size 7 (6 classes plus a 0 class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "layers: Array[Int] = Array(201, 30, 30, 7)\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define layers\n",
    "val layers = Array[Int](201, 30, 30, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainer: org.apache.spark.ml.classification.MultilayerPerceptronClassifier = mlpc_7b54fc8143c4\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define the neural network model\n",
    "val trainer = new MultilayerPerceptronClassifier()\n",
    "    .setLabelCol(\"label\")\n",
    "    .setFeaturesCol(\"comboFeatures1\")\n",
    "    .setLayers(layers)\n",
    "    .setBlockSize(128)\n",
    "    .setSeed(1234L)\n",
    "    .setMaxIter(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel = mlpc_7b54fc8143c4\n"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// train the model - ignore executor memory warnings\n",
    "val model = trainer.fit(df_workingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [id: string, parent_score: bigint ... 13 more fields]\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// get prediction values\n",
    "val predictions = model.transform(df_workingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_ae9c2f993240\n"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// compute metric\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "    .setLabelCol(\"label\")\n",
    "    .setPredictionCol(\"prediction\")\n",
    "    .setMetricName(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Measure: 0.5985587381418374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "myMetric: Double = 0.5985587381418374\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// get metric\n",
    "val myMetric = evaluator.evaluate(predictions)\n",
    "\n",
    "// view metric\n",
    "println(\"F-Measure: \" +  myMetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [Model: string, F-Measure: double]\n"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//update results dataframe\n",
    "results = results.withColumn(\"F-Measure\", when(col(\"Model\").equalTo(\"Multilayer Perceptron Classifier\"), \n",
    "                                               myMetric).otherwise(col(\"F-Measure\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Hour: 13\n",
      "Current Minute: 8\n"
     ]
    }
   ],
   "source": [
    "// get current time\n",
    "println(\"Current Hour: \" + LocalDateTime.now().getHour() + \"\\nCurrent Minute: \" + LocalDateTime.now().getMinute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_83860f050017\n"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define the decision tree model\n",
    "val dt = new DecisionTreeClassifier()\n",
    "    .setLabelCol(\"label\")\n",
    "    .setFeaturesCol(\"comboFeatures1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.classification.DecisionTreeClassificationModel = DecisionTreeClassificationModel (uid=dtc_83860f050017) of depth 5 with 15 nodes\n"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// train the model\n",
    "val model = dt.fit(df_workingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [id: string, parent_score: bigint ... 13 more fields]\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// get prediction values\n",
    "val predictions = model.transform(df_workingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_3bba9175ab5e\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// compute metric\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Measure: 0.6541996533779226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "myMetric: Double = 0.6541996533779226\n"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// get metric\n",
    "val myMetric = evaluator.evaluate(predictions)\n",
    "\n",
    "// view metric\n",
    "println(\"F-Measure: \" +  myMetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [Model: string, F-Measure: double]\n"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//update results dataframe\n",
    "results = results.withColumn(\"F-Measure\", when(col(\"Model\").equalTo(\"Decision Tree\"), \n",
    "                                               myMetric).otherwise(col(\"F-Measure\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Hour: 13\n",
      "Current Minute: 10\n"
     ]
    }
   ],
   "source": [
    "// get current time\n",
    "println(\"Current Hour: \" + LocalDateTime.now().getHour() + \"\\nCurrent Minute: \" + LocalDateTime.now().getMinute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nb: org.apache.spark.ml.classification.NaiveBayes = nb_85e3632cf039\n"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define the naive bayes model\n",
    "val nb = new NaiveBayes()\n",
    "    .setLabelCol(\"label\")\n",
    "    .setFeaturesCol(\"TF-IDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nbModel: org.apache.spark.ml.classification.NaiveBayesModel = NaiveBayesModel (uid=nb_85e3632cf039) with 6 classes\n"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// train the model\n",
    "val nbModel = nb.fit(df_workingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [id: string, parent_score: bigint ... 13 more fields]\n"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// get prediction values\n",
    "val predictions = nbModel.transform(df_workingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_e13dee2431fd\n"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// compute metric\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Measure: 0.05874539353805046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "myMetric: Double = 0.05874539353805046\n"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// get metric\n",
    "val myMetric = evaluator.evaluate(predictions)\n",
    "\n",
    "// view metric\n",
    "println(\"F-Measure: \" +  myMetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [Model: string, F-Measure: double]\n"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//update results dataframe\n",
    "results = results.withColumn(\"F-Measure\", when(col(\"Model\").equalTo(\"Naive Bayes\"), \n",
    "                                               myMetric).otherwise(col(\"F-Measure\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Hour: 13\n",
      "Current Minute: 11\n"
     ]
    }
   ],
   "source": [
    "// get current time\n",
    "println(\"Current Hour: \" + LocalDateTime.now().getHour() + \"\\nCurrent Minute: \" + LocalDateTime.now().getMinute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-------------------+\n",
      "|Model                           |F-Measure          |\n",
      "+--------------------------------+-------------------+\n",
      "|Logistic Regression             |0.5445865731926203 |\n",
      "|Random Forest                   |0.3560933182077977 |\n",
      "|Multilayer Perceptron Classifier|0.5985587381418374 |\n",
      "|Decision Tree                   |0.6541996533779226 |\n",
      "|Naive Bayes                     |0.05874539353805046|\n",
      "+--------------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// view the model results\n",
    "results.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Decision Tree was the best performing model by more than 10% over the next best performing model, logistic regression  \n",
    "\n",
    "**Decision Tree is therefore chosen as the most best performing model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model: Hyperparameter Tuning\n",
    "Hyperparameter tuning of the Decision Tree model using cross-validation and grid search of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Parameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hashingTF2: org.apache.spark.ml.feature.HashingTF = hashingTF_fcba16cf65b9\n",
       "idf2: org.apache.spark.ml.feature.IDF = idf_bc0dced4da12\n",
       "assembler2: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_0521db7c142a\n",
       "dt: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_a176f0300ed1\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_41f262bdf2b0\n"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Define complete decision tree pipeline\n",
    "val hashingTF2 = new HashingTF()\n",
    "    .setInputCol(\"words_stopped\")\n",
    "    .setOutputCol(\"tf2\")\n",
    "var idf2 = new IDF().setInputCol(\"tf2\").setOutputCol(\"idf\")\n",
    "val assembler2 = new VectorAssembler()\n",
    "    .setInputCols(Array(\"parent_score\", \"idf\"))\n",
    "    .setOutputCol(\"comboFeaturesPipe\")\n",
    "val dt = new DecisionTreeClassifier()\n",
    "    .setLabelCol(\"label\")\n",
    "    .setFeaturesCol(\"comboFeaturesPipe\")\n",
    "val pipeline = new Pipeline()\n",
    "    .setStages(Array(hashingTF2, idf2,assembler2,dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tdtc_a176f0300ed1-maxBins: 32,\n",
       "\tdtc_a176f0300ed1-maxDepth: 5,\n",
       "\thashingTF_fcba16cf65b9-numFeatures: 200\n",
       "}, {\n",
       "\tdtc_a176f0300ed1-maxBins: 32,\n",
       "\tdtc_a176f0300ed1-maxDepth: 5,\n",
       "\thashingTF_fcba16cf65b9-numFeatures: 1000\n",
       "}, {\n",
       "\tdtc_a176f0300ed1-maxBins: 64,\n",
       "\tdtc_a176f0300ed1-maxDepth: 5,\n",
       "\thashingTF_fcba16cf65b9-numFeatures: 200\n",
       "}, {\n",
       "\tdtc_a176f0300ed1-maxBins: 64,\n",
       "\tdtc_a176f0300ed1-maxDepth: 5,\n",
       "\thashingTF_fcba16cf65b9-numFeatures: 1000\n",
       "}, {\n",
       "\tdtc_a176f0300ed1-maxBins: 32,\n",
       "\tdtc_a176f0300ed1-maxDepth: 10,\n",
       "\thashingTF_fcba16cf65b9-numFeatures: 200\n",
       "}, {\n",
       "\tdtc_a176f0300ed1-maxBins: 32,\n",
       "\tdtc_a176f0300ed1-maxDepth: 10,\n",
       "\thashingTF_fcba16cf65b9-numFeatures: 1000\n",
       "}, {\n",
       "\tdtc_a176f0300ed1-maxBins: 64,\n",
       "\tdtc_a176f0300ed1-maxDepth: 10,\n",
       "\thashingTF_fcba16c..."
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define grid search parameters\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "    .addGrid(hashingTF2.numFeatures, Array(200, 1000))\n",
    "    .addGrid(dt.maxBins, Array(32,64))\n",
    "    .addGrid(dt.maxDepth, Array(5,10))\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cv: org.apache.spark.ml.tuning.CrossValidator = cv_848afe09fef6\n"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define cross-validation parameters for the pipeline\n",
    "val cv = new CrossValidator()\n",
    "    .setEstimator(pipeline)\n",
    "    .setEvaluator(new MulticlassClassificationEvaluator().setMetricName(\"f1\"))\n",
    "    .setEstimatorParamMaps(paramGrid)\n",
    "    .setNumFolds(3) \n",
    "    .setParallelism(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cvModel: org.apache.spark.ml.tuning.CrossValidatorModel = cv_848afe09fef6\n"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Perform cross-validation for each set of grid search parameters - ignore executor memory warnings\n",
    "val cvModel = cv.fit(df_workingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res37: Array[Double] = Array(0.6465391863739914, 0.6478580969288755, 0.6540975626208687, 0.6554583318367874, 0.6491290332151539, 0.6543520828244637, 0.6516840512229796, 0.6547793771356328)\n"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// view the metric for each set of grid search parameters\n",
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Hour: 15\n",
      "Current Minute: 46\n"
     ]
    }
   ],
   "source": [
    "// get current time\n",
    "println(\"Current Hour: \" + LocalDateTime.now().getHour() + \"\\nCurrent Minute: \" + LocalDateTime.now().getMinute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit Decision Tree Model Using Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [id: string, parent_score: bigint ... 16 more fields]\n"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// get prediction values using the best parameter set from the hyperparameter tuning\n",
    "val predictions = cvModel.transform(df_workingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_f2b9a1b793a6\n"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// compute metric\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Measure: 0.6558979125693123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "myMetric: Double = 0.6558979125693123\n"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// get metric\n",
    "val myMetric = evaluator.evaluate(predictions)\n",
    "\n",
    "// view metric\n",
    "println(\"F-Measure: \" +  myMetric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Best Set of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bestPipelineModel: org.apache.spark.ml.PipelineModel = pipeline_41f262bdf2b0\n",
       "stages: Array[org.apache.spark.ml.Transformer] = Array(hashingTF_fcba16cf65b9, idf_bc0dced4da12, vecAssembler_0521db7c142a, DecisionTreeClassificationModel (uid=dtc_a176f0300ed1) of depth 5 with 23 nodes)\n",
       "hashingStage: org.apache.spark.ml.feature.HashingTF = hashingTF_fcba16cf65b9\n",
       "dtStage: org.apache.spark.ml.classification.DecisionTreeClassificationModel = DecisionTreeClassificationModel (uid=dtc_a176f0300ed1) of depth 5 with 23 nodes\n"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define variables of tuned parameters\n",
    "val bestPipelineModel = cvModel.bestModel.asInstanceOf[PipelineModel]\n",
    "val stages = bestPipelineModel.stages\n",
    "val hashingStage = stages(0).asInstanceOf[HashingTF]\n",
    "val dtStage = stages(3).asInstanceOf[DecisionTreeClassificationModel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final selected decision tree hyperparameters:\n",
      "numFeatures = 1000\n",
      "MaxDepth = 5\n",
      "MaxBins = 64\n"
     ]
    }
   ],
   "source": [
    "// view tuned parameters\n",
    "println(\"Final selected decision tree hyperparameters:\")\n",
    "println(\"numFeatures = \" + hashingStage.getNumFeatures)\n",
    "println(\"MaxDepth = \" + dtStage.getMaxDepth)\n",
    "println(\"MaxBins = \" + dtStage.getMaxBins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res41: String =\n",
       "\"DecisionTreeClassificationModel (uid=dtc_a176f0300ed1) of depth 5 with 23 nodes\n",
       "  If (feature 0 <= 48.5)\n",
       "   Predict: 3.0\n",
       "  Else (feature 0 > 48.5)\n",
       "   If (feature 0 <= 178.5)\n",
       "    If (feature 0 <= 90.5)\n",
       "     If (feature 0 <= 64.5)\n",
       "      Predict: 3.0\n",
       "     Else (feature 0 > 64.5)\n",
       "      If (feature 667 <= 1.5735454947114331)\n",
       "       Predict: 4.0\n",
       "      Else (feature 667 > 1.5735454947114331)\n",
       "       Predict: 3.0\n",
       "    Else (feature 0 > 90.5)\n",
       "     If (feature 0 <= 134.5)\n",
       "      If (feature 667 <= 1.5735454947114331)\n",
       "       Predict: 4.0\n",
       "      Else (feature 667 > 1.5735454947114331)\n",
       "       Predict: 3.0\n",
       "     Else (feature 0 > 134.5)\n",
       "      If (feature 667 <= 1.5735454947114331)\n",
       "       Predict: 5.0\n",
       "      Else (feature 667 > 1.5735454947114331)\n",
       "       Predict: 3.0\n",
       "   Else (feature 0 > 17..."
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// view decision tree branching\n",
    "dtStage.toDebugString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Decision Tree branching using tuned parameters to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res42: java.io.PrintWriter = $anon$1@39009fb3\n"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// export decision tree to text file for review\n",
    "new java.io.PrintWriter(\"./decisionTreeModel.txt\") { write(dtStage.toDebugString); close }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
